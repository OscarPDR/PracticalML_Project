{"name":"Practical ML project","tagline":"Project code for the Practical Machine Learning course from coursera's Data Science specialization (Johns Hopkins University)","body":"# Practical Machine Learning - Project\r\n## Author: Oscar Pe√±a del Rio\r\n## Date: 27/12/2015\r\n\r\nFirst, we load all the libraries we will be using within our analysis, and load the data itself (we suppose the data is present in the same folder as the R script).\r\n\r\n```{r}\r\nlibrary(caret)\r\nlibrary(randomForest)\r\nlibrary(rattle)\r\nlibrary(rpart.plot)\r\nlibrary(RColorBrewer)\r\n\r\ntraining_data <- read.csv('pml-training.csv')\r\ntesting_data <- read.csv('pml-testing.csv')\r\n```\r\n\r\nPrint how many observations and variables we have in each dataset.\r\n\r\n```{r}\r\ndim(training_data)\r\ndim(testing_data)\r\n```\r\n\r\nIn order to remove columns where all observations are NA, we perform the following processing step.\r\n\r\n```{r}\r\ntraining_data <- training_data[, colSums(is.na(training_data)) == 0]\r\ntesting_data <- testing_data[, colSums(is.na(testing_data)) == 0]\r\n```\r\n\r\nWe still have some columns we are not interested in, so we are going to remove some of them manually from the *testing_data* set, and the drop all the columns from *training_data* which are not present in *testing_data* (they are NOT sensor related). We will save the *classe* column and append it afterwards to the *training_data* set.\r\n\r\n```{r}\r\ntesting_data <- subset(testing_data, select=-c(\r\n  X, raw_timestamp_part_1, raw_timestamp_part_2, \r\n  cvtd_timestamp, new_window, num_window, problem_id\r\n))\r\n\r\ndim(testing_data)\r\n\r\ncol_names_to_keep <- colnames(testing_data)\r\nclasse <- training_data$classe\r\n\r\ntraining_data <- training_data[, (colnames(training_data) %in% col_names_to_keep)]\r\ntraining_data$classe <- classe\r\ndim(training_data)\r\n```\r\n\r\nLet's begin our prediction!\r\n\r\nFirst, we create a partition on our *training_data* for cross validation, 70% will form the *train_data* set and the remaining 30% the *test_data* set.\r\n\r\n```{r}\r\n# set seed to today's date\r\nset.seed(20151227)\r\n\r\nfor_training <- createDataPartition(training_data$classe, p=0.7, list=FALSE)\r\n\r\ntrain_data <- training_data[for_training,]\r\ntest_data <- training_data[-for_training,]\r\n```\r\n\r\nFor the prediction model, we will apply the Random Forest algorithm. First, we set the control parameters for the *train()* function, using 10-folds for cross validation.\r\n\r\n```{r}\r\nRF_model <- randomForest(\r\n  train_data$classe ~ .,\r\n  data=train_data,\r\n  ntree=250\r\n)\r\n\r\nRF_model\r\n```\r\n\r\nNow, we build our prediction tool, and check it against the *test_data* set.\r\n\r\n```{r}\r\nRF_prediction <- predict(RF_model, test_data)\r\n\r\nconfusion_matrix <- confusionMatrix(test_data$classe, RF_prediction)\r\n\r\nmodel_accuracy <- postResample(RF_prediction, test_data$classe)\r\nmodel_error <- 1 - as.numeric(confusion_matrix$overall[1])\r\n```\r\n\r\nFinally, we print all the results:\r\n\r\n```{r}\r\nconfusion_matrix\r\nmodel_accuracy\r\nmodel_error\r\n```\r\n\r\nAnd the prettyfied decision tree of the *randomForest()* function:\r\n\r\n```{r}\r\ndecision_tree <- rpart(\r\n  train_data$classe ~ .,\r\n  data=train_data,\r\n  method='class'\r\n)\r\n\r\nfancyRpartPlot(decision_tree)\r\n```\r\n\r\n## Conclusions\r\n\r\nWe have an accuracy of 99.39%, with an error rate of 0.61%. They are quite good results for a *quick* analysis. So we launch it against the initial *testing_data* set, in order to predict their outcome values.\r\n\r\n```{r}\r\nprediction <- predict(RF_model, testing_data)\r\n\r\nprediction\r\n```","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}